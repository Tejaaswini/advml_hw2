\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{balance}
\usepackage{subcaption}
\usepackage{float}
\geometry{margin=0.7in}

\title{News Recommendation Systems: A Comparative Evaluation of Content-Based, User-Based Collaborative Filtering, and Clustering Approaches}

\author{Tejaaswini Narendran}
\date{December 2025}

\begin{document}

\twocolumn[
\begin{@twocolumnfalse}
\maketitle
\begin{abstract}
We present a systematic evaluation of three fundamental recommendation paradigms for news personalization: content-based filtering using TF-IDF, user-based collaborative filtering, and clustering-based methods. Experiments on the MIND dataset reveal a sharp divergence between training and development performance. While content-based filtering achieves strong training results (HR@1: 83.08\%), all approaches converge to similar development accuracy (HR@1: $\sim$10\%) due to severe cold-start conditions. Only 11.9\% of development users appear in training, and the resulting lack of historical signal dominates model behavior. Clustering exhibits the smallest generalization gap (63.2\% relative drop), while content-based filtering provides marginal advantages for unseen articles. These findings highlight that in real-world news platforms, algorithmic sophistication is secondary to addressing cold-start limitations.
\end{abstract}
\end{@twocolumnfalse}
]

\section{Introduction}

News recommendation systems operate under conditions distinct from conventional product recommendation platforms. Articles have short lifespans, user interests shift rapidly, and feedback is sparse. Consequently, models must generalize effectively to users and items unseen during training. To study these constraints, we evaluate three classical recommendation approaches on the MIND dataset: content-based filtering via TF-IDF vectorization, user-based collaborative filtering, and clustering-based recommendations.

A central objective of this study is to assess the generalization behavior of these methods under realistic cold-start conditions. Our results show a pronounced train--development gap across all methods, driven primarily by sparse user overlap between splits. Despite wide variation in training performance, development performance converges, underscoring that robust cold-start handling is essential in operational news recommendation systems.

\section{Methodology}

\subsection{Content-Based Filtering}

Articles are embedded using a TF-IDF vectorizer with 5,000 dimensions, unigram and bigram features, and English stop-word removal. A user profile $\mathbf{u}_i$ for user $i$ is computed as the mean vector of clicked articles:

\begin{equation}
\mathbf{u}_i = \frac{1}{|\mathcal{C}_i|} \sum_{a \in \mathcal{C}_i} \mathbf{v}_a
\end{equation}

where $\mathcal{C}_i$ is the set of articles clicked by user $i$ and $\mathbf{v}_a$ is the TF-IDF vector for article $a$. Recommendations are generated by computing cosine similarity between user profiles and candidate articles. For users without click history, scores are interpolated with article popularity (weight 0.5) to handle cold-start scenarios.

\subsection{User-Based Collaborative Filtering}

A binary user--item interaction matrix $\mathbf{R} \in \{0,1\}^{n \times m}$ is constructed from click histories, where $n$ is the number of users and $m$ is the number of articles. Pairwise cosine similarities between users are computed:

\begin{equation}
\text{sim}(u_i, u_j) = \frac{\mathbf{r}_i \cdot \mathbf{r}_j}{\|\mathbf{r}_i\| \|\mathbf{r}_j\|}
\end{equation}

where $\mathbf{r}_i$ is the interaction vector for user $i$. Recommendations are generated by aggregating clicks from the top-$k$ similar users (we use $k=50$). The score for article $a$ for user $i$ is:

\begin{equation}
s_{ia} = \sum_{j \in \mathcal{N}_i} \text{sim}(u_i, u_j) \cdot \mathbf{R}_{ja}
\end{equation}

where $\mathcal{N}_i$ is the set of top-$k$ most similar users. This method relies entirely on interaction similarity and does not use article content. A popularity fallback (weight 0.3) is used when no similar users have clicked an article.

\subsection{Clustering-Based Recommendations}

Users are grouped via K-Means clustering ($K=50$) on their interaction vectors. Cluster centroids $\boldsymbol{\mu}_k$ minimize the within-cluster sum of squares:

\begin{equation}
\sum_{k=1}^{K} \sum_{u_i \in C_k} \|\mathbf{r}_i - \boldsymbol{\mu}_k\|^2
\end{equation}

where $C_k$ is cluster $k$. Cluster-level article preference distributions are constructed by aggregating articles liked by users within each cluster, normalized by cluster size. Recommendations are generated by matching users to clusters and scoring articles based on cluster preferences. A popularity fallback (weight 0.3) is used when clusters provide no explicit signal for an article.

\section{Experimental Setup}

\subsection{Dataset}

We use the MINDsmall dataset with a standard train--dev split. The training set contains 51,282 articles, 156,965 impressions, and 50,000 users. The development set includes 42,416 articles, 73,152 impressions, and 50,000 users. Critically, only 11.9\% of development users appear in training, and 67.1\% of development articles overlap with training articles. The user--item matrix has density 0.000608, indicating extreme sparsity. This configuration creates an extreme cold-start evaluation environment that tests generalization to both new users and new articles.

\subsection{Evaluation Metrics}

We evaluate using four standard ranking metrics that capture different aspects of recommendation quality:

\textbf{Hit Rate @ 1 (HR@1):} Fraction of impressions where the top-ranked item was clicked. This measures the system's ability to identify the single most relevant article.

\textbf{Mean Reciprocal Rank (MRR):} Average of $1/\text{rank}$ of the first clicked article. This captures ranking quality, rewarding systems that place clicked items higher in the ranking.

\textbf{NDCG @ 5:} Normalized Discounted Cumulative Gain at cutoff 5. This accounts for multiple clicked items with position discount, normalized for comparison across impression sizes.

\textbf{AUC:} Area Under the ROC Curve, measuring discrimination between clicked and non-clicked candidates.

\textbf{Precision @ 5:} Fraction of top-5 recommended articles that were clicked. This measures the accuracy of recommendations at the top-k level.

\textbf{Recall @ 5:} Fraction of clicked articles that appear in the top-5 recommendations. This measures the coverage of relevant items.

\textbf{F1 @ 5:} Harmonic mean of Precision @ 5 and Recall @ 5, providing a balanced measure of recommendation quality.

\section{Results}

Table~\ref{tab:results} summarizes the performance of all three approaches on both training and development sets.

\begin{table*}[t]
\centering
\small
\begin{tabular}{lccccccccc}
\toprule
\textbf{Method} & \textbf{Set} & \textbf{HR@1} & \textbf{MRR} & \textbf{NDCG@5} & \textbf{AUC} & \textbf{Prec@5} & \textbf{Rec@5} & \textbf{F1@5} & \textbf{Coverage} \\
\midrule
Content-Based & Train & 0.8308 & 0.9061 & 0.9887 & 0.9869 & 0.3085 & 0.9970 & 0.4496 & 100.0\% \\
              & Dev   & 0.1056 & 0.2513 & 0.3496 & 0.5386 & 0.0947 & 0.3882 & 0.1489 & 92.9\% \\
\midrule
User-Based CF & Train & 0.6080 & 0.7032 & 0.7752 & 0.8352 & 0.2259 & 0.8143 & 0.3424 & 100.0\% \\
              & Dev   & 0.1002 & 0.2444 & 0.3392 & 0.5345 & 0.0914 & 0.3773 & 0.1440 & 92.1\% \\
\midrule
Clustering    & Train & 0.2721 & 0.3973 & 0.4826 & 0.6370 & 0.1408 & 0.5233 & 0.2159 & 100.0\% \\
              & Dev   & 0.1001 & 0.2444 & 0.3397 & 0.5351 & 0.0915 & 0.3778 & 0.1442 & 92.1\% \\
\bottomrule
\end{tabular}
\caption{Performance comparison on MINDsmall dataset. Training performance varies widely (HR@1: 27.21\%--83.08\%), but development performance converges (HR@1: 10.01\%--10.56\%) due to cold-start dominance. Coverage remains high ($>$92\%) across all methods.}
\label{tab:results}
\end{table*}

\subsection{Training Performance}

Content-based filtering achieves the highest training accuracy (HR@1: 83.08\%, MRR: 0.9061, NDCG@5: 0.9887), indicating that TF-IDF representations effectively capture user preferences when sufficient historical data is available. User-based CF performs moderately (HR@1: 60.80\%, MRR: 0.7032), while clustering yields lower absolute performance (HR@1: 27.21\%, MRR: 0.3973) but benefits from computational scalability and reduced memory requirements.

\subsection{Development Performance}

All three methods show dramatic performance degradation on the development set, with HR@1 dropping to approximately 10\% across all approaches. This convergence strongly correlates with the absence of user history for 88.1\% of development users, forcing all methods to revert to popularity-driven behavior. Content-based filtering maintains a slight edge (HR@1: 10.56\% vs 10.01\%--10.02\%), possibly due to its ability to score new articles through content similarity even without user history. However, personalization is fundamentally limited by missing historical context.

Notably, coverage remains high ($>$92\%) across all methods, indicating that the systems can generate recommendations for most users, though these recommendations are largely non-personalized. Figure~\ref{fig:dev_comparison} visualizes the development performance across all metrics, showing the convergence of performance despite different training accuracies.

\section{Analysis}

\subsection{Generalization Gap}

The relative degradation from training to development performance reveals critical differences in generalization behavior. Content-based filtering shows the largest relative drop (87.3\% decrease in HR@1), suggesting overfitting to training user patterns. User-based CF shows an 83.5\% drop, while clustering shows the smallest relative decrease (63.2\%). Thus, although clustering performs worst on training data, it exhibits the most stable behavior under distribution shift, indicating better robustness to the cold-start scenario.

Figure~\ref{fig:train_dev_comparison} illustrates the train--dev performance gap for each method. The visualization reveals that while training performance varies dramatically (HR@1 ranging from 27\% to 83\%), development performance converges to approximately 10\% across all methods. Figure~\ref{fig:generalization} shows the generalization scores (dev/train ratio) and relative performance drops, confirming that clustering exhibits the most stable generalization behavior despite lower absolute performance.

\subsection{Cold-Start Dominance}

Cold-start effects completely overshadow algorithmic differences. With minimal user overlap between splits (11.9\%), the absence of historical signal renders advanced personalization techniques largely irrelevant. All methods converge to similar performance levels, suggesting that the primary bottleneck is not algorithmic choice but rather the fundamental lack of user interaction data. These findings reinforce that effective news recommendation in production depends primarily on mitigating cold-startâ€”e.g., via hybrid models, demographic metadata, temporal signals, or other side-information sources.

The convergence of development performance across methods, despite their different underlying assumptions, highlights that cold-start handling is the critical challenge rather than the specific recommendation algorithm employed.

\subsection{Method-Specific Insights}

\textbf{Content-Based Filtering:} Performs strongly when user profiles exist, achieving 83\% HR@1 on training data. The approach can rank unseen articles through content similarity, explaining its marginal dev advantage. However, it creates filter bubbles by recommending articles similar to past clicks and shows poor generalization (87.3\% performance drop).

\textbf{User-Based CF:} Limited by extreme sparsity (matrix density: 0.000608) and computational complexity (requiring a 50,000$\times$50,000 similarity matrix). The approach is highly dependent on user overlap to be effective, making it particularly unsuitable for cold-start scenarios. Despite moderate training performance, it generalizes poorly (83.5\% drop).

\textbf{Clustering:} Provides the most stable generalization (63.2\% drop) and computational efficiency, reducing complexity from O($n^2$) to O($n$). However, fixed cluster assignments limit adaptability to evolving user preferences, and cluster quality depends on the choice of $K$. While absolute performance is lower, it offers the best trade-off between performance and robustness.

\section{Conclusion}

Our evaluation of three representative recommendation approaches reveals that development-set performance converges due to extreme cold-start conditions inherent in news personalization. Content-based filtering excels when data is sufficient (83\% HR@1) but generalizes poorly (87.3\% drop). User-based CF suffers from sparsity and scalability issues, while clustering offers the most stable performance across splits despite lower absolute accuracy.

The convergence of development performance across fundamentally different algorithms underscores that cold-start handling, rather than algorithmic sophistication, is the primary challenge in news recommendation systems. Future work should prioritize hybrid architectures combining content and collaborative signals, incorporating temporal dynamics, demographic features, and other side-information to address the cold-start problem. These factors dominate real-world recommendation quality more than the choice of base algorithm.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/dev_performance_comparison.png}
\caption{Development set performance comparison across all metrics.}
\label{fig:dev_comparison_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/dev_heatmap.png}
\caption{Performance heatmap showing metric scores for each method.}
\label{fig:dev_comparison_b}
\end{subfigure}
\caption{Development set performance visualization. (a) Side-by-side bar chart comparing all three methods across seven metrics (HR@k, MRR, NDCG@k, Precision@k, Recall@k, F1@k, Coverage). (b) Heatmap showing performance intensity, with warmer colors (yellow/orange) indicating higher scores. All methods converge to similar performance levels (HR@1 $\approx$ 10\%) despite different training accuracies, highlighting the dominance of cold-start effects.}
\label{fig:dev_comparison}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/train_dev_content_based.png}
\caption{Content-Based}
\label{fig:train_dev_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/train_dev_user_based.png}
\caption{User-Based CF}
\label{fig:train_dev_b}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/train_dev_clustering.png}
\caption{Clustering}
\label{fig:train_dev_c}
\end{subfigure}
\caption{Train vs Dev performance comparison for each method. The gap between training (lighter bars) and development (darker bars) performance illustrates the generalization challenge. Content-based filtering shows the largest gap (87.3\% relative drop in HR@1), while clustering shows the smallest (63.2\% drop), indicating better robustness to distribution shift despite lower absolute training performance.}
\label{fig:train_dev_comparison}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/generalization_analysis.png}
\caption{Generalization analysis showing (left) absolute performance gap (Train - Dev) and (right) generalization score (Dev/Train ratio). The left panel reveals overfitting magnitude, with clustering exhibiting the smallest gaps across most metrics. The right panel shows generalization scores where values closer to 1.0 indicate better generalization. Clustering consistently shows the highest ratios, confirming its robustness to cold-start conditions despite lower absolute training performance.}
\label{fig:generalization}
\end{figure*}

\balance

\end{document}
